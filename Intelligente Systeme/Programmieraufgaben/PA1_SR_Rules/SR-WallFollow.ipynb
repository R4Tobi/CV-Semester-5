{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SR-Wallfollow-Agenten in einer Gitterwelt\n",
    "\n",
    "### Ressourcen: \n",
    "* SREnvironment-Modul für Hintergrundprozesse der Simulation und für die Visualisierung. \n",
    "* Template File mit Tests und Aufgabenstellung\n",
    "\n",
    "### Aufgabe:  \n",
    "* Implementiert einen SR-Agenten der Wände verfolgen kann in dem gegebenen Jupyter-Notebook `SR-WallFollow-Template.ipnyb`\n",
    "* Dazu sollten die Funktionen `sense(self, sensors)` und `action(self, x)` in der Klasse `WallFollowAgent` ergänzt werden. \n",
    "\n",
    "Folgendes Verhalten soll erfüllt werden:\n",
    "* Der SR-Agent soll in den ersten sechs gegebenen Leveln (ohne enge Zwischenräume) eine Wand finden und verfolgen. Der Agent sollte sich in jedem Schritt bewegen, also nicht in zwei aufeinander folgenden Zeitschritten an der gleichen Stelle stehen bleiben. \n",
    "\t\n",
    "* Zusätzlich soll der Agent, sobald er einmal in direkten Nachbarschaft eines blockierten Feldes gewesen ist, immer in der direkten Nachbarschaft zu einem blockierten Feld bleiben. Das bedeutet, dass er das erste gefundene Objekt bzw. Außenwand entlang fahren soll, ohne dessen Kontur wieder zu verlassen. \n",
    "\t\n",
    "* Angenommen Sie dürften den Agenten nun mit einem Gedächtnis ausstatten, bspw. indem Sie die Sensordaten und Aktion aus dem letzten Zeitschritt zwischenspeichern. Wie wäre es nun möglich Ihre Implementierung so zu ändern, dass der Agent auch in Level 7 die Wandverfolgung beherrscht?  \n",
    "\n",
    "Zur Vorbereitung beinhalten die nächsten Zellen des Notebooks Packages die installiert und importiert werden müssen. Es sollten alle Zellen nacheinander ausgeführt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute ONCE if ipympl is not installed yet\n",
    "%pip install ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages and Config\n",
    "%matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"jshtml\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um euren Agenten zu testen haben wir schon diverse Level vorgegeben und euch ein Python Modul `SREnvironment` mitgeliefert. Ihr könnt einen Blick in das Modul werfen, um zu sehen, wie die Gitterwelt und die Simulation oder die Animation funktionieren. Das ist zur Lösung der Aufgabe aber **nicht notwendig!**\n",
    "\n",
    "Jedes Level besteht aus einen Field, einer Matrix aus 0 und 1, wobei 1 ein belegtes Feld, und 0 ein nicht belegtes Feld ist. Bei der Initialisierung eines neuen Levels wird eine Id, ein Name, die Größe, der Starpunkt und ein Flag, ob es geschlossen ist gegeben. \n",
    "\n",
    "Ein nicht geschlossenes Level hat Seitenränder die überschritten werden können. Das Level _\"loopt\"_ dann, das heißt, der Agent kommt aus der anderen Seite wieder raus. \n",
    "\n",
    "Ihr könnt dieses Verhalten in Level 1 Testen, in dem ihr die Default Action von `self.north` zu `self.west` oder `self.east` ändert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from is_sr import Simulation, Renderer, Environment\n",
    "from is_sr.Levels import Level1, Level2, Level3, Level4, Level5, Level6, Level7, levels\n",
    "\n",
    "Level0 = Environment(0, \"Seperate Rooms\", (12,12), ((3,5), (7,5)))\n",
    "Level0.field[ 0, :] = 1\n",
    "Level0.field[-1, :] = 1\n",
    "Level0.field[ :, 0] = 1\n",
    "Level0.field[ :,-1] = 1\n",
    "Level0.field[ 6, :] = 1\n",
    "Level0.field[:3,:3] = 1\n",
    "Level0.field[-5:,-4:] = 1\n",
    "Level0.field[5,7] = 1\n",
    "Level0.field[1:3,7] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der Klasse `Agent8M` sind Basis Funktionen für einen Agenten mit 8 Sensoren gegeben. Außerdem sind einige Attribute gegeben, die dir bei der Bestimmung der nächsten Aktion helfen können:\n",
    "\n",
    "```\n",
    "east  = np.array(( 1, 0), dtype = np.intp)\n",
    "south = np.array(( 0,-1), dtype = np.intp)\n",
    "west  = np.array((-1, 0), dtype = np.intp)\n",
    "north = np.array(( 0, 1), dtype = np.intp)\n",
    "\n",
    "actions = {\"E\": east, \"S\": south, \"W\": west, \"N\": north}\n",
    "\n",
    "sensors = np.array((\n",
    "              (-1, 1), # NW\n",
    "              ( 0, 1), # N\n",
    "              ( 1, 1), # NE\n",
    "              ( 1, 0), # E\n",
    "              ( 1,-1), # SE\n",
    "              ( 0,-1), # S\n",
    "              (-1,-1), # SW\n",
    "              (-1, 0)  # W\n",
    "            ), dtype=np.intp)\n",
    "```\n",
    "\n",
    "Diese Variablen definieren die Kommandos \"E\", \"S\", \"W\" und \"N\", welche über das Dictonary `actions` in  Bewegungsrichtungen übersetzt werden. Die `update` Funktion muss in jedem Schritt eines dieser Kommandos zurück geben. Das `sensors` Array definiert die Position der Sensoren, daher beginnen wir wie in der Vorlesung oben links ( Nordwesten NW) und numerieren die Sensoren im Uhrzeigersinn.\n",
    "\n",
    "Dein eigener Agent soll von dieser Klasse erben und die Funktionalität so erweitern, dass er in der Lage ist einer Wand in einer Gitterwelt zu folgen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from is_sr.Agents import Agent8M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die folgende Klasse beschreibt den Agenten, der die Aufgabe lösen soll. Wir haben euch schon eine Struktur der Funktion gegeben, die ihr aber natürlich auch erweitern könnt. \n",
    "\n",
    "Die Funktion `sense(...)` erhält die 8 Sensoreingaben als Inputs. Diese sind von `0` bis `7` in der gleichen Reihenfolge wie in der Vorlesung nummeriert, wobei `sensor[i] == 0` wenn das Feld *i* frei ist, und `sensor[i] == 1` wenn es mit einer Wand belegt ist. Aus den Sensordaten werden die *j* Merkmale `x[j]` aus der Vorlesung bestimmt und zurückgegeben. Wie groß *j* ist, also wie viele Merkmale bestimmt werden, hängt von eurer implementierung ab.\n",
    "\n",
    "Die Funktion `action(...)` bekommt die Merkmale aus `sense(...)` und soll eine der vier möglichen Aktionen `self.east`, `self.west`, `self.north` oder `self.sout` zurückgeben. Um euch ein Beispiel zu liefern haben wir die default Rückgabe `return self.north` gegeben. Damit ist die nächste Aktion nach Norden zu gehen. In der Update-Funktion wird die von euch gewählte Aktion ausgeführt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class WallFollowAgent(Agent8M):\n",
    "    \"\"\"\n",
    "    Wall-Following Agent according to lecture\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, memory=False):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        \n",
    "        Defers construction to Base Class\n",
    "        \"\"\"\n",
    "        Agent8M.__init__(self, memory)\n",
    "\n",
    "    @classmethod\n",
    "    def sense(cls, sensors):\n",
    "        \"\"\"\n",
    "        Generate features from environment using sensor input sensors\n",
    "        s0 s1 s2\n",
    "        s7 WF s3\n",
    "        s6 s5 s4\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        STUDENT CODE HERE\n",
    "        \"\"\"\n",
    "        \n",
    "        return ()\n",
    "\n",
    "    @classmethod\n",
    "    def action(cls, x, memory = None):\n",
    "        \"\"\"\n",
    "        Select appropriate rule from rule set based on passed feature vector x\n",
    "\n",
    "        In some levels memory may be allowed and will be filled with the last executed action.\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        STUDENT CODE HERE\n",
    "        \"\"\"\n",
    "        \n",
    "        return \"N\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der nächsten Zelle wird die Simulation und die damit einhergehende Animation erstellt. Bei der Initialisierung der Simulation legt ihr das Level, den Agenten und die Anzahl der Schritte fest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Level-Auswahl: Einfach euer aktuell gewünschtes Level an die Simulation weitergeben\n",
    "level = Level6\n",
    "sim = Simulation(level, WallFollowAgent(level.tight), 100)\n",
    "\n",
    "sensors, cmds, trajectory = sim.run()\n",
    "ani=Renderer.RenderSimulation(sim, trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In den letzten Zellen des Notebooks findet ihr die Testklasse, die auf allen Levels testet, ob ihr die Anforderungen erfüllt habt. \n",
    "\n",
    "Nutzt die Outputs um eure Implementierung zu überprüfen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from is_sr.Tests import runTests\n",
    "\n",
    "runTests(levels, WallFollowAgent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
